{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure nltk\n",
    "\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "nltk_data_path = \"assets/nltk_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load text data from a file and produce a list of token lists\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    \n",
    "    with open('../Sonnet_ngram/THE_SONNETS.txt') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().lower()    \n",
    "            if len(line) > 3:\n",
    "                sentences.append(nltk.word_tokenize(line))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Take a list of sentences and return a vocab\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    for i in sentences:\n",
    "        for k in i:\n",
    "            vocab.add(k)\n",
    "    vocab.add('<s>')\n",
    "    vocab.add('</s>')\n",
    "    \n",
    "    return list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngrams(n, sentences):\n",
    "    \"\"\"\n",
    "    Take a list of unpadded sentences and create all n-grams as specified by the argument \"n\" for each sentence\n",
    "    \"\"\"\n",
    "    all_ngrams = []\n",
    "    for i in sentences:\n",
    "        paddedline = nltk.lm.preprocessing.pad_both_ends(i, n)\n",
    "        ngrams = nltk.ngrams(paddedline, n)\n",
    "        #print(list(ngrams))\n",
    "        ngrams= list(ngrams)\n",
    "        all_ngrams.append(ngrams)\n",
    "    \n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_next_token(start_tokens=(\"<s>\", ) * 3):\n",
    "    \"\"\"\n",
    "    Take some starting tokens and produce the most likely token that follows under a bi-gram model\n",
    "    \"\"\"\n",
    "    \n",
    "    lst = []\n",
    "    for i in build_ngrams(2, load_data()):\n",
    "        for k in i:\n",
    "            if k[0] == start_tokens[-1]:\n",
    "                lst.append(k[1])\n",
    "    next_token = max(lst, key=lst.count)\n",
    "    prob = lst.count(next_token)/len(lst)\n",
    "\n",
    "    \n",
    "    return next_token, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "\n",
    "def train_ngram_lm(n):\n",
    "    \"\"\"\n",
    "    Train a n-gram language model as specified by the argument \"n\"\n",
    "    \"\"\"\n",
    "    \n",
    "    lm = MLE(n)\n",
    "    lm.fit(build_ngrams(n, load_data()), build_vocab(load_data()))\n",
    "    \n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that my steeled sense or changes right or\n",
      "thus vainly thinking that she thinks me young\n",
      "for then despite of space i would be\n",
      "let me excuse thee , ah my love\n",
      "thy worth the greater being wooed of time\n",
      "when sparkling stars twire not thou gildâ€™st the\n",
      "if thou wouldst use the strength of all\n",
      "in our two loves there is but one\n",
      "sets down her babe and makes all swift\n",
      "than when her mournful hymns did hush the\n",
      "although in me each part will be forgotten\n",
      "to that sweet thief which sourly robs from\n",
      "when proud-pied april ( dressed in all his\n",
      "made old offences of affections new .\n"
     ]
    }
   ],
   "source": [
    "# Every time it runs, depending on how drunk it is, a different sonnet is written. \n",
    "n = 4\n",
    "num_lines = 14\n",
    "num_words_per_line = 8\n",
    "text_seed = [\"<s>\"] * (n - 1)\n",
    "\n",
    "lm = train_ngram_lm(n)\n",
    "\n",
    "sonnet = []\n",
    "while len(sonnet) < num_lines:\n",
    "    while True:  # keep generating a line until success\n",
    "        try:\n",
    "            line = lm.generate(num_words_per_line, text_seed=text_seed)\n",
    "        except ValueError:  # the generation is not always successful. need to capture exceptions\n",
    "            continue\n",
    "        else:\n",
    "            line = [x for x in line if x not in [\"<s>\", \"</s>\"]]\n",
    "            sonnet.append(\" \".join(line))\n",
    "            break\n",
    "\n",
    "# pretty-print your sonnet\n",
    "print(\"\\n\".join(sonnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
